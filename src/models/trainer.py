from src.data.dataset import AudioDataset
from src.data.preprocess import normalize_text
from src.data.utils import collate_fn_wrapper
from src.models.utils import import_gigaam_model, get_model_vocab_idx2char, get_model_vocab_char2idx
from src.utils.utils import calculate_wer
#from gigaam.gigaam.preprocess import FeatureExtractor
from src.models.utils import get_gigaam_logprobs, get_texts_idxs
from src.utils.freeze_weights import (
    freeze_model_completely,
    freeze_model_selective,
    freeze_by_components
)

import logging

import os
import torch
import torch.nn as nn
import torch
from torch.utils.data import DataLoader
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from typing import Dict
from pathlib import Path
from tqdm import tqdm
from torch.utils.tensorboard import SummaryWriter
import json
import torch.nn.functional as F
import warnings

# turn off the UserWarnings because lots of them are talking about
# library function refactoring, last or future deprecations
warnings.filterwarnings('ignore', category=UserWarning)

class GigaAMTrainer:
    """
    –¢—Ä–µ–Ω–µ—Ä –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π GigaAM
    """
   
    def __init__(
        self,
        model_type: str = "ssl",  # –∏–ª–∏ "ctc", "rnnt" –¥–ª—è fine-tuning —É–∂–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        output_dir: str = "./checkpoints",
        learning_rate: float = 1e-4,
        warmup_steps: int = 1000,
        max_steps: int = 100000,
        max_epochs: int = 10,
        batch_size: int = 1,
        accumulation_steps: int = 4,
        save_steps: int = 5000,
        eval_steps: int = 1000,
        fp16: bool = True,
        num_workers: int = 4,
    ):
        self.model_type = model_type
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
       
        self.learning_rate = learning_rate
        self.warmup_steps = warmup_steps

        self.max_steps = max_steps
        self.batch_size = batch_size
        self.accumulation_steps = accumulation_steps
        self.save_steps = save_steps
        self.eval_steps = eval_steps
        self.num_workers = num_workers
        self.max_epochs = max_epochs

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
        self.logging_steps: int = 100
        self.save_total_limit: int = 5

        self.setup_logging()

        # –õ–æ–≥–≥–µ—Ä—ã
        self.setup_loggers()
       
        # –°—á–µ—Ç—á–∏–∫–∏
        self.global_step = 0
        self.current_epoch = 0
        self.best_val_loss = float('inf')
       
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.logger.info(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞: {self.device}")
        self.logger.info(f"–ö–æ–ª–∏—á–µ—Å–≤–æ —ç–ø–æ—Ö: {self.max_epochs}")
       
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        self.logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ gigaam —Ç–∏–ø–∞ {model_type}...")
        self.model = import_gigaam_model(model_type=self.model_type, device=self.device)

        self.model.to(self.device)

        self.BLANK_IDX = 33
        self.criterion = nn.CTCLoss(blank=self.BLANK_IDX, reduction='mean', zero_infinity=True)

        # –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–µ–º –≤–µ—Å–∞
        # freeze_model_completely(self.model)
        # freeze_model_selective(self.model)
        # freeze_by_components(self.model)

        self.print_frozen_stats()
       
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ (–±—É–¥–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ train)
        self.optimizer = None
        self.scheduler = None
       
        # –°—á–µ—Ç—á–∏–∫–∏
        self.global_step = 0
        self.current_epoch = 0

    def setup_logging(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
        log_file = self.output_dir / "training.log"

        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)

        # –°–æ–∑–¥–∞–Ω–∏–µ —Ñ–∞–π–ª–æ–≤–æ–≥–æ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.INFO)

        # –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(formatter)

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∞ –∫ –ª–æ–≥–≥–µ—Ä—É
        self.logger.addHandler(file_handler)
   
    def setup_loggers(self):
        """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ TensorBoard"""
        self.tb_writer = None
       
        tb_dir = self.output_dir / "tensorboard"
        tb_dir.mkdir(exist_ok=True)
        self.tb_writer = SummaryWriter(tb_dir)
        self.logger.info(f"TensorBoard –ª–æ–≥–∏: {tb_dir}")

    def log_metrics(self, metrics: Dict, step: int, prefix: str = ""):
        # –ö–æ–Ω—Å–æ–ª—å
        metrics_str = " - ".join([f"{k}: {v:.4f}" if isinstance(v, float) else f"{k}: {v}"
                                 for k, v in metrics.items()])
        self.logger.info(f"–®–∞–≥ {step} [{prefix}] - {metrics_str}")
       
        # TensorBoard - –¥–æ–±–∞–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥–æ–π –º–µ—Ç—Ä–∏–∫–∏ –æ—Ç–¥–µ–ª—å–Ω–æ
        if self.tb_writer:
            for metric_name, metric_value in metrics.items():
                # –°–æ–∑–¥–∞–µ–º –ø–æ–ª–Ω–æ–µ –∏–º—è –º–µ—Ç—Ä–∏–∫–∏: prefix/metric_name
                full_name = f"{prefix}/{metric_name}" if prefix else metric_name
               
                # –î–æ–±–∞–≤–ª—è–µ–º –≤ TensorBoard
                if isinstance(metric_value, (int, float)):
                    self.tb_writer.add_scalar(full_name, metric_value, step)
                    # –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ: –ª–æ–≥–∏—Ä—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∫–æ–Ω—Å–æ–ª—å –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                    if step % 100 == 0:  # –ö–∞–∂–¥—ã–µ 100 —à–∞–≥–æ–≤
                        self.logger.debug(f"TensorBoard: {full_name} = {metric_value:.4f}(step {step})")
           
            # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —Ñ–ª–µ—à–∏—Ä—É–µ–º –ø–∏—Å–∞—Ç–µ–ª—å –¥–ª—è –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
            self.tb_writer.flush()

    def setup_optimizer(self):
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        self.optimizer = AdamW(
            trainable_params,
            lr=self.learning_rate,
            eps=1e-8,
            weight_decay=0.01
        )

        steps_per_epoch = len(self.train_loader) // self.accumulation_steps
        if len(self.train_loader) % self.accumulation_steps != 0:
            steps_per_epoch += 1
       
        # Cosine annealing scheduler —Å warmup
        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=1e-4,
            epochs=self.max_epochs,
            steps_per_epoch=steps_per_epoch,
            # total_steps=(len(self.train_loader) // self.accumulation_steps) * self.max_epochs,
            pct_start=0.1
        )

    def train_step(self, batch) -> float:
        wav_batch, wav_lengths, targets, target_lengths, texts = batch

        wav_batch = wav_batch.to(self.device)
        wav_lengths = wav_lengths.to(self.device)
        targets = targets.to(self.device)
        target_lengths = target_lengths.to(self.device)

        # –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥
        features, feat_lengths = self.model.preprocessor(wav_batch, wav_lengths)
        encoded, encoded_len = self.model.encoder(features, feat_lengths)
        logprobs = self.model.head(encoded)

        # Log softmax –¥–ª—è CTC
        log_probs = torch.nn.functional.log_softmax(logprobs, dim=-1)
        
        # –ü–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–ª—è CTC: (T, B, C)
        log_probs = log_probs.transpose(0, 1)  # (T, B, C)
        
        # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –¥–ª–∏–Ω—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã
        input_lengths = encoded_len.clamp(min=1)
        target_lengths = target_lengths.clamp(min=1)
      
        # CTC Loss
        loss = self.criterion(log_probs, targets, input_lengths, target_lengths)
       
        (loss / self.accumulation_steps).backward()

        return loss.item()
    
    def train(self):
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤
        self.logger.info("–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...")

        char2idx = get_model_vocab_char2idx(self.model)

        train_dataset = AudioDataset(dataset_part="train", normalize_fn=normalize_text)
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            collate_fn=lambda x: collate_fn_wrapper(x, char2idx),
            # pin_memory=True if torch.cuda.is_available() else False
        )
       
        val_dataset = AudioDataset(dataset_part="validation", normalize_fn=normalize_text)
        self.val_loader = DataLoader(
            val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            collate_fn=lambda x: collate_fn_wrapper(x, char2idx),
            # pin_memory=True if torch.cuda.is_available() else False
        )
       
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞
        self.setup_optimizer()

        self.logger.info("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è...")
        self.logger.info(f"–ë–∞—Ç—á —Ä–∞–∑–º–µ—Ä: {self.batch_size}")
        self.logger.info(f"–ì—Ä–∞–¥–∏–µ–Ω—Ç–Ω–∞—è –∞–∫–∫—É–º—É–ª—è—Ü–∏—è: {self.accumulation_steps}")
        self.logger.info(f"–≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –±–∞—Ç—á —Ä–∞–∑–º–µ—Ä: {self.batch_size * self.accumulation_steps}")
       
        running_loss = 0.0
        self.optimizer.zero_grad()

        self.best_wer = float('inf')
       
        while self.current_epoch < self.max_epochs and self.global_step < self.max_steps:
            self.model.train()
            self.current_epoch += 1
            self.optimizer.zero_grad()
           
            pbar = tqdm(self.train_loader, desc=f"–≠–ø–æ—Ö–∞ {self.current_epoch}")
            for step, batch in enumerate(pbar):
                loss = self.train_step(batch)
                running_loss += loss
               
                # Gradient accumulation
                if (step + 1) % self.accumulation_steps == 0:
                    # Gradient clipping
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.optimizer.step()
                    self.scheduler.step()
                   
                    self.optimizer.zero_grad()
                    self.global_step += 1
                   
                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                    if self.global_step % self.logging_steps == 0:
                        avg_loss = running_loss / self.accumulation_steps
                        lr = self.optimizer.param_groups[0]['lr']
                    
                        metrics = {
                            'loss': avg_loss,
                            'lr': lr,
                        }
                        self.log_metrics(metrics, self.global_step, "train")
                        pbar.set_postfix(metrics)
                        running_loss = 0.0
                
                    # –í–∞–ª–∏–¥–∞—Ü–∏—è
                    if self.global_step % self.eval_steps == 0:
                        self.logger.info(f"–í–∞–ª–∏–¥–∞—Ü–∏—è –Ω–∞ —à–∞–≥–µ {self.global_step}...")
                        self.validate(self.val_loader)
                
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
                    #! temporarry comment this line, cause of no so much free space on my test device
                    # if self.global_step % self.save_steps == 0:
                    #     self.save_checkpoint()

                    running_loss = 0.0

            self.validate(self.val_loader)
           
            if self.global_step >= self.max_steps:
                break
       
        self.logger.info("–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ —á–µ–∫–ø–æ–∏–Ω—Ç–∞...")
        self.save_checkpoint(name="final_model")
        self.logger.info("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")

    def validate(self, val_loader: DataLoader) -> float:
        """
        –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
       
        Args:
            val_loader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
           
        Returns:
            —Å—Ä–µ–¥–Ω–∏–π loss –Ω–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        """
        self.model.eval()

        idx2char = get_model_vocab_idx2char(self.model)
        wer, refs, hyps = calculate_wer(self.model, val_loader, self.device, idx2char, self.BLANK_IDX)

        if wer < self.best_wer:
            self.best_wer = wer
            checkpoint_path = os.path.join(self.output_dir, f'best_model_wer_{wer*100:.2f}.pt')
            torch.save({
                'epoch': self.current_epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict(),
                'wer': wer,
                # 'loss': avg_loss,
                'vocab': self.model.decoding.tokenizer.vocab,
                # 'char_to_idx': char_to_idx,
                'blank_idx': self.BLANK_IDX,  # –°–æ—Ö—Ä–∞–Ω—è–µ–º blank index
            }, checkpoint_path)

        self.log_metrics({"wer": wer}, self.global_step, "validation")

        self.model.train()
        return wer
    
    def save_checkpoint(self, name: str = None):
        """
        –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ –º–æ–¥–µ–ª–∏
       
        Args:
            name: –∏–º—è —á–µ–∫–ø–æ–∏–Ω—Ç–∞ (–µ—Å–ª–∏ None, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è global_step)
        """
        if name is None:
            name = f"checkpoint_step_{self.global_step}"
       
        checkpoint_path = self.output_dir / name
        checkpoint_path.mkdir(parents=True, exist_ok=True)
       
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ state dict –º–æ–¥–µ–ª–∏
        '''
        torch.save({
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'global_step': self.global_step,
            'epoch': self.current_epoch,
        }, checkpoint_path / "pytorch_model.bin")
        '''
        torch.save(self.model.state_dict(), checkpoint_path / 'gigaam_weights.pth')
       
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        config = {
            'model_name': self.model_type,
            'global_step': self.global_step,
            'epoch': self.current_epoch,
            'learning_rate': self.learning_rate,
        }
       
        with open(checkpoint_path / "training_config.json", 'w') as f:
            json.dump(config, f, indent=2)
       
        self.logger.info(f"–ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {checkpoint_path}")

    def print_frozen_stats(self):
        total_params = 0
        frozen_params = 0
        
        for name, param in self.model.named_parameters():
            total_params += param.numel()
            if not param.requires_grad:
                frozen_params += param.numel()
                self.logger.info(f"‚ùå –ó–∞–º–æ—Ä–æ–∂–µ–Ω: {name}")
            else:
                self.logger.info(f"‚úÖ –†–∞–∑–º–æ—Ä–æ–∂–µ–Ω: {name}")
        
        self.logger.info(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
        self.logger.info(f"–í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
        self.logger.info(f"–ó–∞–º–æ—Ä–æ–∂–µ–Ω–æ: {frozen_params:,} ({frozen_params/total_params*100:.1f}%)")
        self.logger.info(f"–û–±—É—á–∞–µ—Ç—Å—è: {total_params-frozen_params:,} ({(total_params-frozen_params)/total_params*100:.1f}%)")

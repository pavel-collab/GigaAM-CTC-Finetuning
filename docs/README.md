# Отчет о задаче по дообучению модели GigaAM-CTC на FLEURS-Ru

В данном задании мы взяли open source модель GigaAM-CTC и дообучили на распознавание речи на русскоязычной части датасета FLEURS-Ru.
Основная цель: достигнуть показателя Word Error Rate (WER далее) < 8%.

## Предобработка данных

Поскольку в словаре модели содержатся только буквы русского алфавита, а в транскрипциях датасета присутствует множество
других символов таких как:
- знаки препинания
- арабские и римские числа
- буквы латинского алфавита

все эти символы будут вносить ошибку при распознавании и подсчете WER. Поэтому был разработан пайплайн предобработки 
транскрипций в датасете. Изначально это была простая функция, в последствии возможности предобработки были расширены, что
отразилось на конечных результатах (см. раздел результаты).

## Дообучение

Для дообучения я написал упрощенный аналог Trainer из библиотеки transformers (или библиотеки pytorch_lightning), реализовав только основные механики.

## Результаты

Так как замеры производились с двумя разными функциями предобработки транскрипций исходного датасета, будем дополнительно
в этом разделе помечать, с какой функцией были получены результаты. Обозначим
- fun1 -- более простая функция предобработки
- fun2 -- более полная функция предобработки
По умолчанию, если не указано обратного, считаем, что результаты получены с применением fun1.

Baseline модель (до обучения на датасете) давала показатель __WER = 0.0698__ с применением fun1 и __WER = 0.0488__ с применением fun2. После этого мы дообучили ее на различном количестве эпох с разными функциями препроцессинга.

### Результаты обучения с простой функцией препроцессинга на различном количестве эпох

Эволюция ctc loss на тренировке и изменение learning rate.

![training loss и learning rate](/docs/images/train_simple_preprocessing.png)

Эволюция wer на валидации.

![Эволюция wer на валидации](/docs/images/wer_simple_preprocessing.png)

### Результаты обучения с улучшенной функцией препроцессинга на различном количестве эпох

Эволюция ctc loss на тренировке и изменение learning rate.

![training loss и learning rate](/docs/images/training_boost_preprocessing.png)

Эволюция wer на валидации.

![Эволюция wer на валидации](/docs/images/wer_boost_preprocessing.png)

### Результаты обучения с последовательным замораживанием слоев модели

Эволюция wer на валидации

![Эволюция wer на валидации](/docs/images/wer_freezing.png)

### Сводные результаты тестирования

![Сравнительная оценка модели с различными функциями препроцессинга](/docs/images/results_per_epochs.png)

![Сравнительная оценка wer на модели с заморозкой слоев](/docs/images/results_per_freezing.png)
# Отчет о задаче по дообучению модели GigaAM-CTC на FLEURS-Ru

В данном задании мы взяли open source модель GigaAM-CTC и дообучили на распознавание речи на русскоязычной части датасета FLEURS-Ru.
Основная цель: достигнуть показателя Word Error Rate (WER далее) < 8%.

## Предобработка данных

Поскольку в словаре модели содержатся только буквы русского алфавита, а в транскрипциях датасета присутствует множество
других символов таких как:
- знаки препинания
- арабские и римские числа
- буквы латинского алфавита

все эти символы будут вносить ошибку при распознавании и подсчете WER. Поэтому был разработан пайплайн предобработки 
транскрипций в датасете. Изначально это была простая функция, в последствии возможности предобработки были расширены, что
отразилось на конечных результатах (см. раздел результаты).

## Дообучение

Для дообучения я написал упрощенный аналог Trainer из библиотеки transformers (или библиотеки pytorch_lightning), реализовав только основные механики.

## Результаты

Так как замеры производились с двумя разными функциями предобработки транскрипций исходного датасета, будем дополнительно
в этом разделе помечать, с какой функцией были получены результаты. Обозначим
- fun1 -- более простая функция предобработки
- fun2 -- более полная функция предобработки
По умолчанию, если не указано обратного, считаем, что результаты получены с применением fun1.

Baseline модель (до обучения на датасете) давала показатель __WER = 0.0698__ с применением fun1 и __WER = 0.0488__ с применением fun2.

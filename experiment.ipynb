{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BVUSRBbKzHe",
        "outputId": "78c7f868-21a5-48cc-9be5-77417a802ca1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets==3.6.0 in /usr/local/lib/python3.12/dist-packages (3.6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (3.6.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (0.35.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets==3.6.0) (6.0.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets==3.6.0) (1.1.10)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets==3.6.0) (2025.10.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets==3.6.0) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets==3.6.0) (1.22.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.6.0) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install datasets==3.6.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --no-deps git+https://github.com/salute-developers/GigaAM.git pywer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqLTANVYKz6c",
        "outputId": "3d4f8e2c-f67b-4b2f-a09b-344d18b800d2"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/salute-developers/GigaAM.git\n",
            "  Cloning https://github.com/salute-developers/GigaAM.git to /tmp/pip-req-build-pdt03mei\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/salute-developers/GigaAM.git /tmp/pip-req-build-pdt03mei\n",
            "  Resolved https://github.com/salute-developers/GigaAM.git to commit 6a8b511f753670ed38af6529bb89bbdc2191ba6a\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pywer in /usr/local/lib/python3.12/dist-packages (0.1.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset"
      ],
      "metadata": {
        "id": "_JNUFKFEK312"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fleurs = load_dataset(\"google/fleurs\", \"ru_ru\")"
      ],
      "metadata": {
        "id": "9DqUGvZnK57C"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "from typing import Dict"
      ],
      "metadata": {
        "id": "zswSpFGcMjZC"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, dataset, dataset_part: str=\"train\"):\n",
        "       self.dataset = dataset\n",
        "\n",
        "       if dataset_part == \"train\":\n",
        "        self.dataset = self.dataset['train']\n",
        "       elif dataset_part == \"validation\":\n",
        "        self.dataset = self.dataset['validation']\n",
        "       else:\n",
        "          self.dataset = self.dataset['test']\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict:\n",
        "        sample = self.dataset[idx]\n",
        "\n",
        "        return {\n",
        "            'audio': torch.tensor(sample['audio']['array'], dtype=torch.float32),\n",
        "            'num_samples': torch.tensor(sample['num_samples'], dtype=torch.int32),\n",
        "            'transcription': sample['transcription'],\n",
        "        }"
      ],
      "metadata": {
        "id": "tR6gtHU1MZaf"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "    # Для полей с разной длиной (например, audio) нужно добавить паддинг\n",
        "    audio = [item['audio'] for item in batch]\n",
        "    audio_padded = torch.nn.utils.rnn.pad_sequence(audio, batch_first=True)\n",
        "\n",
        "    return audio_padded, torch.stack([item['num_samples'] for item in batch]), [item['transcription'] for item in batch]"
      ],
      "metadata": {
        "id": "DLovJ75HMu4g"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = AudioDataset(fleurs, dataset_part='train')"
      ],
      "metadata": {
        "id": "6s6Qh9lAM7Yu"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=4,\n",
        "            shuffle=True,\n",
        "            num_workers=1,\n",
        "            collate_fn=collate_fn\n",
        "        )"
      ],
      "metadata": {
        "id": "PJfJunqRMyXI"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "  audio, num_samples, texts = batch\n",
        "  print(audio)\n",
        "  print(num_samples)\n",
        "  print(texts)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RWNNMClhNFQM",
        "outputId": "e56a9a23-17dd-431b-8c86-badcc2c8e28a"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ..., -2.2054e-06,\n",
            "          4.4703e-06, -4.4703e-06],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "          0.0000e+00,  0.0000e+00]])\n",
            "tensor([163200, 142080, 224640, 122880], dtype=torch.int32)\n",
            "['видимость также может быть ограничена вследствие снегопада метели конденсата или льда на стёклах транспортного средства', 'несмотря на эти обвинения ма легко победил выступая за более тесные связи с материковой частью китая', 'во время этих чудовищных штормов бушуют ветра со скоростью до 480 км/ч 133 м/с; 300 миль в час', 'впоследствии эдинбургский шерифский суд предъявил адекое обвинение в убийстве ее сына']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install hydra-core"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zho_JX7OJSa",
        "outputId": "207ec3fa-0f4b-4744-a413-7f9c6a046270"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hydra-core in /usr/local/lib/python3.12/dist-packages (1.3.2)\n",
            "Requirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.12/dist-packages (from hydra-core) (2.3.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from hydra-core) (4.9.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from hydra-core) (25.0)\n",
            "Requirement already satisfied: PyYAML>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from omegaconf<2.4,>=2.2->hydra-core) (6.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "13byid2SOK-u"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gigaam import GigaAMASR\n",
        "import gigaam\n",
        "\n",
        "CACHE_DIR = os.path.expanduser(\"~/.cache/gigaam\")\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_name, model_path = gigaam._download_model('ctc', CACHE_DIR)\n",
        "\n",
        "ckpt = torch.load(model_path, map_location='cpu', weights_only=False)\n",
        "\n",
        "ckpt[\"cfg\"].encoder.flash_attn = False\n",
        "model = GigaAMASR(ckpt['cfg'])\n",
        "\n",
        "model.load_state_dict(ckpt[\"state_dict\"], strict=False)\n",
        "model = model.eval()\n",
        "\n",
        "if device.type != \"cpu\":\n",
        "  model.encoder = model.encoder.half()\n",
        "\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "dTqToxbZOCcw"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List"
      ],
      "metadata": {
        "id": "x3aqaiwROW_t"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def convert_arabic_number_to_words(number_str):\n",
        "    \"\"\"\n",
        "    Преобразует арабское число в слова\n",
        "    \"\"\"\n",
        "    try:\n",
        "        num = int(number_str)\n",
        "    except ValueError:\n",
        "        return number_str\n",
        "\n",
        "    # Базовые словари\n",
        "    units = ['', 'один', 'два', 'три', 'четыре', 'пять', 'шесть', 'семь', 'восемь', 'девять']\n",
        "    teens = ['десять', 'одиннадцать', 'двенадцать', 'тринадцать', 'четырнадцать',\n",
        "             'пятнадцать', 'шестнадцать', 'семнадцать', 'восемнадцать', 'девятнадцать']\n",
        "    tens = ['', '', 'двадцать', 'тридцать', 'сорок', 'пятьдесят',\n",
        "            'шестьдесят', 'семьдесят', 'восемьдесят', 'девяносто']\n",
        "    hundreds = ['', 'сто', 'двести', 'триста', 'четыреста', 'пятьсот',\n",
        "                'шестьсот', 'семьсот', 'восемьсот', 'девятьсот']\n",
        "\n",
        "    if num == 0:\n",
        "        return 'ноль'\n",
        "\n",
        "    words = []\n",
        "\n",
        "    # Обрабатываем тысячи\n",
        "    if num >= 1000:\n",
        "        thousands = num // 1000\n",
        "        if thousands == 1:\n",
        "            words.append('тысяча')\n",
        "        elif thousands in [2, 3, 4]:\n",
        "            words.append(units[thousands] + ' тысячи')\n",
        "        else:\n",
        "            words.append(convert_arabic_number_to_words(str(thousands)) + ' тысяч')\n",
        "        num %= 1000\n",
        "\n",
        "    # Обрабатываем сотни\n",
        "    if num >= 100:\n",
        "        words.append(hundreds[num // 100])\n",
        "        num %= 100\n",
        "\n",
        "    # Обрабатываем десятки и единицы\n",
        "    if num >= 20:\n",
        "        words.append(tens[num // 10])\n",
        "        if num % 10 > 0:\n",
        "            words.append(units[num % 10])\n",
        "    elif num >= 10:\n",
        "        words.append(teens[num - 10])\n",
        "    elif num > 0:\n",
        "        words.append(units[num])\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "def convert_roman_number_to_words(roman_str):\n",
        "    \"\"\"\n",
        "    Преобразует римское число в слова\n",
        "    \"\"\"\n",
        "    roman_numerals = {\n",
        "        'I': 1, 'V': 5, 'X': 10, 'L': 50,\n",
        "        'C': 100, 'D': 500, 'M': 1000\n",
        "    }\n",
        "\n",
        "    roman_str = roman_str.upper()\n",
        "    total = 0\n",
        "    prev_value = 0\n",
        "\n",
        "    for char in reversed(roman_str):\n",
        "        if char not in roman_numerals:\n",
        "            return roman_str\n",
        "\n",
        "        value = roman_numerals[char]\n",
        "        if value < prev_value:\n",
        "            total -= value\n",
        "        else:\n",
        "            total += value\n",
        "        prev_value = value\n",
        "\n",
        "    return convert_arabic_number_to_words(str(total))\n",
        "\n",
        "def replace_latin_with_russian(text):\n",
        "    \"\"\"\n",
        "    Заменяет латинские буквы на похожие русские\n",
        "    \"\"\"\n",
        "    latin_to_russian = {\n",
        "        'a': 'а', 'b': 'б', 'c': 'к', 'd': 'д', 'e': 'е',\n",
        "        'f': 'ф', 'g': 'г', 'h': 'х', 'i': 'и', 'j': 'й',\n",
        "        'k': 'к', 'l': 'л', 'm': 'м', 'n': 'н', 'o': 'о',\n",
        "        'p': 'п', 'q': 'к', 'r': 'р', 's': 'с', 't': 'т',\n",
        "        'u': 'у', 'v': 'в', 'w': 'в', 'x': 'кс', 'y': 'у', 'z': 'з'\n",
        "    }\n",
        "\n",
        "    for latin, russian in latin_to_russian.items():\n",
        "        text = text.replace(latin, russian)\n",
        "        text = text.replace(latin.upper(), russian.upper())\n",
        "\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Предобрабатывает текст по заданным правилам:\n",
        "    1. Оставляет только русские буквы в нижнем регистре\n",
        "    2. Заменяет знаки препинания на пробелы\n",
        "    3. Заменяет ё на е\n",
        "    4. Преобразует числа в слова\n",
        "    5. Заменяет английские буквы на русские\n",
        "    \"\"\"\n",
        "\n",
        "    # Шаг 1: Заменяем ё на е\n",
        "    text = text.replace('ё', 'е').replace('Ё', 'е')\n",
        "\n",
        "    # Шаг 2: Заменяем латинские буквы на русские\n",
        "    text = replace_latin_with_russian(text)\n",
        "\n",
        "    # Шаг 3: Преобразуем римские цифры\n",
        "    # Ищем римские цифры (от I до MMMCMXCIX)\n",
        "    roman_pattern = r'\\b[IVXLCDM]+\\b'\n",
        "    text = re.sub(roman_pattern, lambda m: convert_roman_number_to_words(m.group()), text, flags=re.IGNORECASE)\n",
        "\n",
        "    # Шаг 4: Преобразуем арабские цифры\n",
        "    # Ищем целые числа\n",
        "    arabic_pattern = r'\\b\\d+\\b'\n",
        "    text = re.sub(arabic_pattern, lambda m: convert_arabic_number_to_words(m.group()), text)\n",
        "\n",
        "    # Шаг 5: Заменяем все знаки препинания на пробелы\n",
        "    punctuation_chars = string.punctuation + '—–«»„“‚‘'\n",
        "    for char in punctuation_chars:\n",
        "        text = text.replace(char, ' ')\n",
        "\n",
        "    # Шаг 6: Приводим к нижнему регистру\n",
        "    text = text.lower()\n",
        "\n",
        "    # Шаг 7: Удаляем все символы, кроме русских букв и пробелов\n",
        "    text = re.sub(r'[^а-я\\s]', '', text)\n",
        "\n",
        "    # Шаг 8: Убираем лишние пробелы\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "ZAtXdz2AOY2-"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model_vocab(model):\n",
        "    model_vocab = {sym: idx for idx, sym in enumerate(model.decoding.tokenizer.vocab)}\n",
        "    return model_vocab\n",
        "\n",
        "def pad_list(nested_list, padding_element=33):\n",
        "    max_len = max(len(sublist) for sublist in nested_list)\n",
        "\n",
        "    padded_list = [\n",
        "        sublist + [padding_element] * (max_len - len(sublist))\n",
        "        for sublist in nested_list\n",
        "    ]\n",
        "\n",
        "    return padded_list\n",
        "\n",
        "def get_texts_idxs(texts: List[str], model_vocab: Dict[str, str], blank_token=33) -> torch.Tensor:\n",
        "  texts_idxs = []\n",
        "\n",
        "  for text in texts:\n",
        "    text = preprocess_text(text)\n",
        "\n",
        "    text_idxs = list([model_vocab[sym] for sym in text])\n",
        "    texts_idxs.append(text_idxs)\n",
        "\n",
        "  texts_idxs = pad_list(texts_idxs, padding_element=blank_token)\n",
        "  return torch.tensor(texts_idxs, dtype=torch.int)\n",
        "\n",
        "def get_gigaam_logprobs(model, wav_batch, wav_lengths, return_transcriptions=False):\n",
        "    wav_batch = wav_batch.to(model._device)\n",
        "    wav_lengths = wav_lengths.to(model._device)\n",
        "\n",
        "    encoded, encoded_len = model.forward(wav_batch, wav_lengths)\n",
        "\n",
        "    logprobs = model.head(encoded)\n",
        "\n",
        "    if return_transcriptions:\n",
        "        transcriptions = model.decoding.decode(model.head, encoded, encoded_len)\n",
        "        return logprobs, encoded_len, transcriptions\n",
        "    else:\n",
        "        return logprobs, encoded_len"
      ],
      "metadata": {
        "id": "k-q0SO9yOQKP"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "VKwZBPenOs4M"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _compute_ctc_loss(logprobs, encoded_len, transcripts, transcript_lengths):\n",
        "  # Проверяем и выравниваем длины\n",
        "  encoded_len = tuple(encoded_len.to('cpu').numpy())\n",
        "\n",
        "  # Убеждаемся, что encoded_len не превышает длину logprobs по времени\n",
        "  T = logprobs.size(1)  # временная размерность после transpose\n",
        "  encoded_len = tuple(min(el, T) for el in encoded_len)\n",
        "\n",
        "  # CTCLoss требует логиты в формате (T, N, C)\n",
        "  logprobs = logprobs.transpose(0, 1)  # Теперь форма (T, N, C)\n",
        "\n",
        "  BLANK_IDX = 33\n",
        "  ctc_loss = nn.CTCLoss(blank=BLANK_IDX, reduction='mean', zero_infinity=True)\n",
        "\n",
        "  # Вычисляем потерю\n",
        "  loss = ctc_loss(\n",
        "      logprobs,           # (T, N, C)\n",
        "      transcripts,        # (N, S) -> целочисленные индексы\n",
        "      encoded_len,        # (N,) -> длины выходных последовательностей\n",
        "      transcript_lengths  # (N,) -> длины целевых последовательностей\n",
        "  )\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "Lym1n6vAOmOS"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in train_loader:\n",
        "   audios, audio_lengths, texts = batch\n",
        "\n",
        "   model_vocab = get_model_vocab(model)\n",
        "\n",
        "   #TODO: maybe move it to the dataloader?\n",
        "   texts = get_texts_idxs(texts, model_vocab)\n",
        "\n",
        "   transcript_lengths=(len(sample) for sample in texts)\n",
        "\n",
        "   logprobs, encoded_len = get_gigaam_logprobs(model, audios, audio_lengths)\n",
        "\n",
        "   loss =  _compute_ctc_loss(\n",
        "                logprobs,\n",
        "                encoded_len,\n",
        "                texts,\n",
        "                transcript_lengths=tuple(transcript_lengths)\n",
        "            )\n",
        "\n",
        "   print(loss)\n",
        "   break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYqh5fCONorY",
        "outputId": "ed7a01a1-c653-455c-90a1-520ca52617f0"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.6494, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    }
  ]
}